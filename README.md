# An Introduction to Knowledge Distillation

This repository provides a clear and accessible introduction to the concept of **Knowledge Distillation (KD)**. It contains educational materials designed to bridge the gap between theory and practice, making it easy for anyone to learn how to train smaller, more efficient models by "distilling" knowledge from larger, more complex ones.

The materials were prepared for a university project to introduce this powerful model compression technique.

## Prerequisites

The content is designed for those with a basic understanding of machine learning and neural networks. Familiarity with Python and PyTorch is also beneficial, as the examples are implemented using these technologies.

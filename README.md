# A Practical Introduction to Knowledge Distillation

This repository provides a clear and accessible introduction to the concept of **Knowledge Distillation (KD)**. It contains educational materials designed to bridge the gap between theory and practice, making it easy for anyone to learn how to train smaller, more efficient models by "distilling" knowledge from larger, more complex ones.

The materials were prepared for a university project to introduce this powerful model compression technique.

---

## Goal of the Project

The primary goal of this repository is to offer simple, practical, and easy-to-digest materials for anyone getting started with Knowledge Distillation.

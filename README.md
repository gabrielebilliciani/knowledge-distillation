# An Introduction to Knowledge Distillation

This repository provides a clear and accessible introduction to the concept of **Knowledge Distillation (KD)**. It contains educational materials designed to bridge the gap between theory and practice, making it easy for anyone to learn how to train smaller, more efficient models by "distilling" knowledge from larger, more complex ones.

The materials were prepared for a university project to introduce this powerful model compression technique.

## Prerequisites

The content is designed for those with a basic understanding of machine learning and neural networks. Familiarity with Python and PyTorch is also beneficial, as the examples are implemented using these technologies.

## Structure
The repositoryâ€™s structure is straightforward:
- Slides: A set of slides providing an overview of KD concepts and techniques, building from the basics.
- Notebooks: Interactive Jupyter notebooks that demonstrate KD in practice, including:
  - **Notebook 1**: Regression tasks using KD.
  - **Notebook 2**: Classification tasks with MLPs on MNIST.
  - **Notebook 3**: Classification tasks with CNNs and ResNets on CIFAR10 and CIFAR100.

Plase be warned that some of the notebooks might take a long time to run, especially those involving larger models or datasets. Those provided here already contain execution results, so you can view the outcomes without running the code again.
